<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Detail in reproducing VolSDF</title>
      <link href="/2022/04/19/Detail-in-reproducing-VolSDF/"/>
      <url>/2022/04/19/Detail-in-reproducing-VolSDF/</url>
      
        <content type="html"><![CDATA[<p>This post mainly records the reproducing detail and helps me reproduce VolSDF more efficiently.</p><span id="more"></span><h2 id="volume-rendering">Volume Rendering</h2><h3 id="signed-distence-function">Signed Distence Function</h3><p><span class="math display">\[\mathbf{1}_{\Omega}(\boldsymbol{x})=\left\{\begin{array}{ll}1 &amp; \text { if } \boldsymbol{x} \in \Omega \\0 &amp; \text { if } \boldsymbol{x} \notin \Omega\end{array}, \quad \text { and } d_{\Omega}(\boldsymbol{x})=(-1)^{\mathbf{1}_{\Omega}(\boldsymbol{x})} \min _{\boldsymbol{y} \in \mathcal{M}}\|\boldsymbol{x}-\boldsymbol{y}\|\right.\]</span></p><p>In VolSDF, <span class="math inline">\(d_{\Omega}(\boldsymbol{x})\)</span> is estimated by network directly.</p><blockquote><p>To satisfy the assumption that all rays, including rays that do not intersect any surface, are eventually occluded (i.e., O(<span class="math inline">\(\infty\)</span>) = 1), we model our SDF as: <span class="math display">\[d_\Omega(\boldsymbol x) = \min \{d(\boldsymbol x), r - ||x||_2\}\]</span></p></blockquote><h3 id="density-sigma">Density <span class="math inline">\(\sigma\)</span></h3><p><span class="math display">\[\sigma(\boldsymbol{x})=\alpha \Psi_{\beta}\left(-d_{\Omega}(\boldsymbol{x})\right)\]</span></p><p>And,</p><p><span class="math display">\[\begin{aligned}    \Psi_{\beta}(s) &amp;= \begin{cases}\frac{1}{2} \exp \left(\frac{s}{\beta}\right) &amp; \text { if } s \leq 0 \\ 1-\frac{1}{2} \exp \left(-\frac{s}{\beta}\right) &amp; \text { if } s&gt;0\end{cases} \\    &amp;= -\text{sign}(s)\cdot \frac{1}{2}\exp(-\frac{|s|}{\beta}) + \frac{1+\text{sign}(s)}{2} \end{aligned}\]</span></p><p>where <span class="math inline">\(\alpha, \beta &gt; 0\)</span> are learnable parameters, in experiments, <span class="math inline">\(\alpha = \frac{1}{\beta}\)</span></p><h3 id="rendering-procedure">Rendering Procedure</h3><p>The rendering function is the same as the one in NeRF,</p><p><span class="math display">\[I(\boldsymbol{c}, \boldsymbol{v})=\int_{0}^{\infty} L(\boldsymbol{x}(t), \boldsymbol{n}(t), \boldsymbol{v}) \tau(t) d t\]</span></p><p>And <span class="math display">\[\tau(t)=\frac{d O}{d t}(t)=\sigma(\boldsymbol{x}(t)) T(t)\]</span></p><p><span class="math display">\[T(t)=\exp \left(-\int_{0}^{t} \sigma(\boldsymbol{x}(s)) d s\right)\]</span> <span class="math inline">\(\sigma\)</span> is the density, <span class="math inline">\(T\)</span> is the probility that the ray travels from tn to t without hitting any other particle.</p><p>The integral is approximated usign a numerical quadrature, namely the rectangle rule: <span class="math display">\[I(\boldsymbol{c}, \boldsymbol{v}) \approx \hat{I}_{\mathcal{S}}(\boldsymbol{c}, \boldsymbol{v})=\sum_{i=1}^{m-1} \hat{\tau}_{i} L_{i}\]</span></p><p><span class="math display">\[\begin{aligned}I(\boldsymbol{c}, \boldsymbol{v}) &amp;=\int_{0}^{\infty} L(\boldsymbol{x}(t), \boldsymbol{n}(t), \boldsymbol{v}) \tau(t) d t \\&amp;=\int_{0}^{M} L(\boldsymbol{x}(t), \boldsymbol{n}(t), \boldsymbol{v}) \tau(t) d t+\int_{M}^{\infty} L(\boldsymbol{x}(t), \boldsymbol{n}(t), \boldsymbol{v}) \tau(t) d t \\&amp; \approx \sum_{i=1}^{m-1} \delta_{i} \tau\left(s_{i}\right) L_{i}\end{aligned}\]</span> <span class="math inline">\(T\)</span> is also a integral, so we use the same method to estimate it. <span class="math display">\[T\left(s_{i}\right) \approx \hat{T}\left(s_{i}\right)=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)\]</span></p><p><span class="math display">\[p_i = \exp(-\sigma_i\delta_i)\]</span> Then , <span class="math inline">\(\hat{T}(s_i)=\Pi_{j=1}^{i-1}p_j\)</span>, estimate <span class="math inline">\(\sigma_i\delta_i\)</span> <span class="math display">\[\sigma_i\delta_i \approx (1-\exp(-\sigma_i\delta_i)) = 1-p_i\]</span> when <span class="math inline">\(\sigma_i\delta_i\)</span> is very small, the loss can be omitted.</p><center><img src="https://blog-image-zxy.oss-cn-hangzhou.aliyuncs.com/sigma_approx_f5d5850c.png" width="60%"> <br><div style="color:orange;solid #d9d9d9;    display: inline-block;    color: #999;    padding: 1px;"><b>Blue line for δσ, red line for 1 - p</b></div></center><p>So the <span class="math inline">\(\tau\)</span> can be written as following: <span class="math display">\[\hat{\tau}_{i}=\left(1-p_{i}\right) \prod_{j=1}^{i-1} p_{j} \quad \text { for } 1 \leq i \leq m-1, \quad \text { and } \hat{\tau}_{m}=\prod_{j=1}^{m-1} p_{j}\]</span> And <span class="math display">\[I(\boldsymbol{c}, \boldsymbol{v}) \approx \hat{I}(\boldsymbol{c}, \boldsymbol{v})=\sum_{i=1}^{m-1}\left[\left(1-p_{i}\right) \prod_{j=1}^{i-1} p_{j}\right] L_{i}=\sum_{i=1}^{m-1}\left[\left(1-\exp(-\sigma_i\delta_i)\right) \exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j)\right] L_{i}\]</span></p><p>Compared with NeRF:</p><p><span class="math display">\[\hat{C}(\mathbf{r})=\sum_{i=1}^{N} T_{i}\left(1-\exp \left(-\sigma_{i} \delta_{i}\right)\right) \mathbf{c}_{i}, \text { where } T_{i}=\exp \left(-\sum_{j=1}^{i-1} \sigma_{j} \delta_{j}\right)\]</span></p><p>You will find they are very similar.</p><hr /><h2 id="network-architecture">Network Architecture</h2>There're two networks and one is geometry network <span class="math inline">\(\boldsymbol{f}_\varphi\)</span> with 8-layer, 256-width MLP and a single skip connection from the input to the 4th layer.<center><img src="https://blog-image-zxy.oss-cn-hangzhou.aliyuncs.com/2022-04-19-15-30-57_02aeb7c2.png"/><br><div style="color:orange;solid #d9d9d9;    display: inline-block;    color: #999;    padding: 1px;"><b>The architecture of geometry network</b></div></center><center><img src="https://blog-image-zxy.oss-cn-hangzhou.aliyuncs.com/2022-04-19-15-39-53_421d799f.png"/><br><div style="color:orange;solid #d9d9d9;    display: inline-block;    color: #999;    padding: 1px;"><b>The architecture of radience field network</b></div></center><ul><li><p>VolSDF also uses positional encoding, but one thing we should pay attention is the geometry network only encodes the position <span class="math inline">\(\boldsymbol{x}\)</span> and radiance field network only encodes view direction <span class="math inline">\(\boldsymbol{v}\)</span>. Although the radiance field network also takes in position <span class="math inline">\(\boldsymbol{x}\)</span>, it doesn't encode it.</p></li><li><p>The geometry network uses softplus as activation function while the radiance field network uses ReLU as activation function.</p></li></ul><hr /><h2 id="loss-function">Loss Function</h2><p>Like NeRF, VolSDF uses two network, one for density, and one for color. We can define the loss as the distence between rendered color and expected color. But this loss is strongly related to the radiance field, and we need add another loss term to train the geometry network. And we notice that the L2 distence (<span class="math inline">\(d_\Omega(\boldsymbol{x})\)</span>) has a gradient of length 1. So the loss for the geometry network is <span class="math inline">\(\mathbb{E}(||\nabla_x d|| - 1)\)</span></p><p>The loss function of VolSDF is:</p><p><span class="math display">\[\mathcal{L}(\theta)=\mathcal{L}_{\mathrm{RGB}}(\theta)+\lambda \mathcal{L}_{\mathrm{SDF}}(\varphi), \quad\text{where}\]</span> <span class="math display">\[\mathcal{L}_{\mathrm{RGB}}(\theta)=\mathbb{E}_{p}\left\|I_{p}-\hat{I}_{\mathcal{S}}\left(\boldsymbol{c}_{p}, \boldsymbol{v}_{p}\right)\right\|_{1}, \quad \text { and } \mathcal{L}_{\mathrm{SDF}}(\varphi)=\mathbb{E}_{\boldsymbol{z}}(\|\nabla d(\boldsymbol{z})\|-1)^{2} \text {, }\]</span></p><p><span class="math inline">\(\mathcal{L}_{\mathrm{SDF}}\)</span> is the Eikonal loss, and the samples <span class="math inline">\(\boldsymbol z\)</span> are taken to combine a single random uniform space point and a single point from <span class="math inline">\(S\)</span> for each pixel <span class="math inline">\(p\)</span>. <span class="math inline">\(\lambda\)</span> is a hyper-parameter and set 0.1 in experiments.</p><p>Notice that, this loss mainly serve the training the geometry network and not for rendering. So we don't compute the gradient of the function in <a href="#signed-distence-function">Signed Distence Function</a> , <span class="math inline">\(d\)</span> not <span class="math inline">\(d_\Omega\)</span></p><hr /><h2 id="sampling-algorithm">Sampling Algorithm</h2><center><img src="https://blog-image-zxy.oss-cn-hangzhou.aliyuncs.com/2022-04-19-21-23-10_03bbed4c.png" width="50%"/><br><div style="color:orange;solid #d9d9d9;    display: inline-block;    color: #999;    padding: 1px;"><b>Algorithm </b></div></center><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Paper Reading: NeRF(Nerual Radience Field)</title>
      <link href="/2022/04/03/Paper-Reading-NeRF-Nerual-Radience-Field/"/>
      <url>/2022/04/03/Paper-Reading-NeRF-Nerual-Radience-Field/</url>
      
        <content type="html"><![CDATA[<figure><img src="https://blog-image-zxy.oss-cn-hangzhou.aliyuncs.com/2022-04-18-23-42-32_3eea9ec2.png" alt="NeRF" /><figcaption>NeRF</figcaption></figure><h2 id="introduction">Introduction</h2><p>NeRF is to solve the problem of view sythesis by directly optimizing parameters of a continuous 5D function, and it uses neural network to represent scenes and store the information of a real model. The neural network has a 5D input, the spatial location <span class="math inline">\((x, y, z)\)</span> and viewing direction <span class="math inline">\((\theta, \phi)\)</span>, and outputs the volume density and the view-dependent emitted radiance at that spatial location. The volume density is only related with the spatial location, while radiance is also related with the viewing direction. It is because we can see different color from different view, like mirrors.</p><span id="more"></span><h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2><p>The output can be seen as a vector in a 3D point, and there's a vector field called neural radiance field.</p><p>The input is 3D location <span class="math inline">\({\bf{x}}=(x,y,z)\)</span> and 2D view direction <span class="math inline">\((\theta, \phi)\)</span> which also can be represented as 3D Cartesian unit vector <span class="math inline">\({\bf{d}}\)</span>. The structure of the network is extremely simple and is an 8-layer MLP with a skip connection at 4th layer using ReLU activations and 256 channels per layer. The output is <span class="math inline">\(\sigma\)</span> which is the density, and the 256-dimension feature. Then the feature vector is concatenated with view direction, and then fed into another MLP which outputs the view-dependent RGB color.</p><h2 id="volume-rendering">Volume Rendering</h2><p>What we want to do is synthesize novol view only by camera intrinsics and extrinsics. But the input is not the parameters.</p><p>Actually, the color of each pixel can be seen as how much radiance arrive at the image plane along the ray that emits from the camera and goes through the specific pixel.</p><p>So we can formulate the color of camera as</p><p><span class="math display">\[C({\bf{r}}) = \int_{t_n}^{t_f}  T(t)\sigma({\bf r}(t)){\bf c}({\bf r}(t),{\bf d}) \mathrm{d}t, \text{where}\ T(t) = \exp(-\int_{t_n}^t\sigma({\bf r}(s))\  \mathrm{d}s)\]</span></p><p>The function <span class="math inline">\(T(t)\)</span> denotes the accumulated transmittance along the ray from <span class="math inline">\(t_n\)</span> to <span class="math inline">\(t\)</span>, the probility taht the ray travels from <span class="math inline">\(t_n\)</span> to <span class="math inline">\(t\)</span> without hitting any other particle. If there's <span class="math inline">\(t\)</span> such that <span class="math inline">\(\sigma\)</span> is large, <span class="math inline">\(T\)</span> will drop drastically, which is as following.</p><figure><img src="https://blog-image-zxy.oss-cn-hangzhou.aliyuncs.com/T_3d1944d4.png" alt="The red line is T, the blue is sigma" /><figcaption>The red line is T, the blue is sigma</figcaption></figure><p>We need to estimate the above fucntion using a numerical method. And we sample the rays discretize the integral, and use left Riemann sum. So the integral can be represented as</p><p><span class="math display">\[\hat{C}({\bf{r}}) = \sum_{i=1}^N T_i(1-exp(-\sigma_i\delta_i)){\bf c}_i,\ \text{where}\ T_i = \exp(-\sum_{j=1}^{i-1} \sigma_j\delta_j)\]</span></p><h2 id="two-tricks-in-nerf">Two Tricks in NeRF</h2><h3 id="positional-encoding">Positional Encoding</h3><p>The input is a 5D coordiante, and it is too diffical to use this 5D data direcly estimate the geometry information especially the high-frequency information. They use positional encoding to map the original data to a higher dimension to enable better fitting of data that contains high frequency variation</p><p><span class="math display">\[\gamma(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\]</span></p><p><span class="math display">\[F_{\Theta} = F_{\Theta}&#39;\circ\gamma\]</span></p><p>We also see positional encoding in Transformer, a model for NLP. Although they have the same name and function, they serve different purposes. In Transformer, it is to add positional information, because attention doesn't take position into consider, and here, it is to map the data into higher dimension to easily approximate a higher frequency function. ### Hierarchical Sampling</p><p>In volume rendering, we approximate the integral by sampling points along the ray. The more sample points, the more precise it is. But it will also lead to higher overhead. So the samping strategy is important.</p><p>Hierarchical sampling is a sampling strategy from coarse to fine. Uniformly sampling will treat each interval equally, however, we need sample more points in the interval that the volume is occupied by the object.</p><p>They use two network, "coarse" and "fine". First, we use uniform sampling and feed in coarse network, <span class="math display">\[\hat{C_c}({\bf r}) = \sum_{i=1}^{N_c}w_ic_i,\ w_i=T_i(1-\exp(-\sigma_i \delta_i))\]</span></p><p>then normalize the weight. After that, use inverse sample to get another sample sets.</p><h2 id="loss-function">Loss Function</h2><p>The input is 5D coordinate and output is color, and actually we have already know the color of the integral. So the loss is the distence between estimated color and expected color.</p><p>Due to the architecture of dual network, we have to add the color from the coarse network to train easiler.</p><p>So the loss function is</p><p><span class="math display">\[\mathcal{L}=\sum_{\mathbf{r} \in \mathcal{R}}\left[\left\|\hat{C}_{c}(\mathbf{r})-C(\mathbf{r})\right\|_{2}^{2}+\left\|\hat{C}_{f}(\mathbf{r})-C(\mathbf{r})\right\|_{2}^{2}\right]\]</span></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Representation implicitly </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
