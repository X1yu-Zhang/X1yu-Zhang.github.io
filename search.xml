<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Paper Reading: NeRF(Nerual Radience Field)</title>
      <link href="/2022/04/03/Paper-Reading-NeRF-Nerual-Radience-Field/"/>
      <url>/2022/04/03/Paper-Reading-NeRF-Nerual-Radience-Field/</url>
      
        <content type="html"><![CDATA[<figure><img src="https://blog-image-zxy.oss-cn-hangzhou.aliyuncs.com/2022-04-18-23-42-32_3eea9ec2.png" alt="NeRF" /><figcaption>NeRF</figcaption></figure><h2 id="introduction">Introduction</h2><p>NeRF is to solve the problem of view sythesis by directly optimizing parameters of a continuous 5D function, and it uses neural network to represent scenes and store the information of a real model. The neural network has a 5D input, the spatial location <span class="math inline">\((x, y, z)\)</span> and viewing direction <span class="math inline">\((\theta, \phi)\)</span>, and outputs the volume density and the view-dependent emitted radiance at that spatial location. The volume density is only related with the spatial location, while radiance is also related with the viewing direction. It is because we can see different color from different view, like mirrors.</p><span id="more"></span><h2 id="neural-radiance-field-scene-representation">Neural Radiance Field Scene Representation</h2><p>The output can be seen as a vector in a 3D point, and there's a vector field called neural radiance field.</p><p>The input is 3D location <span class="math inline">\({\bf{x}}=(x,y,z)\)</span> and 2D view direction <span class="math inline">\((\theta, \phi)\)</span> which also can be represented as 3D Cartesian unit vector <span class="math inline">\({\bf{d}}\)</span>. The structure of the network is extremely simple and is an 8-layer MLP with a skip connection at 4th layer using ReLU activations and 256 channels per layer. The output is <span class="math inline">\(\sigma\)</span> which is the density, and the 256-dimension feature. Then the feature vector is concatenated with view direction, and then fed into another MLP which outputs the view-dependent RGB color.</p><h2 id="volume-rendering">Volume Rendering</h2><p>What we want to do is synthesize novol view only by camera intrinsics and extrinsics. But the input is not the parameters.</p><p>Actually, the color of each pixel can be seen as how much radiance arrive at the image plane along the ray that emits from the camera and goes through the specific pixel.</p><p>So we can formulate the color of camera as</p><p><span class="math display">\[C({\bf{r}}) = \int_{t_n}^{t_f}  T(t)\sigma({\bf r}(t)){\bf c}({\bf r}(t),{\bf d}) \mathrm{d}t, \text{where}\ T(t) = \exp(-\int_{t_n}^t\sigma({\bf r}(s))\  \mathrm{d}s)\]</span></p><p>The function <span class="math inline">\(T(t)\)</span> denotes the accumulated transmittance along the ray from <span class="math inline">\(t_n\)</span> to <span class="math inline">\(t\)</span>, the probility taht the ray travels from <span class="math inline">\(t_n\)</span> to <span class="math inline">\(t\)</span> without hitting any other particle. If there's <span class="math inline">\(t\)</span> such that <span class="math inline">\(\sigma\)</span> is large, <span class="math inline">\(T\)</span> will drop drastically, which is as following.</p><figure><img src="https://blog-image-zxy.oss-cn-hangzhou.aliyuncs.com/T_3d1944d4.png" alt="The red line is T, the blue is sigma" /><figcaption>The red line is T, the blue is sigma</figcaption></figure><p>We need to estimate the above fucntion using a numerical method. And we sample the rays discretize the integral, and use left Riemann sum. So the integral can be represented as</p><p><span class="math display">\[\hat{C}({\bf{r}}) = \sum_{i=1}^N T_i(1-exp(-\sigma_i\delta_i)){\bf c}_i,\ \text{where}\ T_i = \exp(-\sum_{j=1}^{i-1} \sigma_j\delta_j)\]</span></p><h2 id="two-tricks-in-nerf">Two Tricks in NeRF</h2><h3 id="positional-encoding">Positional Encoding</h3><p>The input is a 5D coordiante, and it is too diffical to use this 5D data direcly estimate the geometry information especially the high-frequency information. They use positional encoding to map the original data to a higher dimension to enable better fitting of data that contains high frequency variation</p><p><span class="math display">\[\gamma(p) = (sin(2^0\pi p), cos(2^0\pi p), \cdots, sin(2^{L-1}\pi p), cos(2^{L-1}\pi p))\]</span></p><p><span class="math display">\[F_{\Theta} = F_{\Theta}&#39;\circ\gamma\]</span></p><p>We also see positional encoding in Transformer, a model for NLP. Although they have the same name and function, they serve different purposes. In Transformer, it is to add positional information, because attention doesn't take position into consider, and here, it is to map the data into higher dimension to easily approximate a higher frequency function. ### Hierarchical Sampling</p><p>In volume rendering, we approximate the integral by sampling points along the ray. The more sample points, the more precise it is. But it will also lead to higher overhead. So the samping strategy is important.</p><p>Hierarchical sampling is a sampling strategy from coarse to fine. Uniformly sampling will treat each interval equally, however, we need sample more points in the interval that the volume is occupied by the object.</p><p>They use two network, "coarse" and "fine". First, we use uniform sampling and feed in coarse network, <span class="math display">\[\hat{C_c}({\bf r}) = \sum_{i=1}^{N_c}w_ic_i,\ w_i=T_i(1-\exp(-\sigma_i \delta_i))\]</span></p><p>then normalize the weight. After that, use inverse sample to get another sample sets.</p><h2 id="loss-function">Loss Function</h2><p>The input is 5D coordinate and output is color, and actually we have already know the color of the integral. So the loss is the distence between estimated color and expected color.</p><p>Due to the architecture of dual network, we have to add the color from the coarse network to train easiler.</p><p>So the loss function is</p><p><span class="math display">\[\mathcal{L}=\sum_{\mathbf{r} \in \mathcal{R}}\left[\left\|\hat{C}_{c}(\mathbf{r})-C(\mathbf{r})\right\|_{2}^{2}+\left\|\hat{C}_{f}(\mathbf{r})-C(\mathbf{r})\right\|_{2}^{2}\right]\]</span></p><link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Representation implicitly </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
